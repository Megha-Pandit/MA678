---
title: "MA678 homework 01"
author: "Meghamala Pandit"
date: "Septemeber 6, 2018"
output:
  pdf_document: default
---

\newcommand{\mat}[1]{\boldsymbol{#1}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\rv}[1]{\underline{#1}} 

```{r setup, include=FALSE}
pacman::p_load(ggplot2, knitr, arm, data.table,Cairo)
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE)
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```

# Introduction 

For homework 1 you will fit linear regression models and interpret them. You are welcome to transform the variables as needed.  How to use `lm` should have been covered in your discussion session.  Some of the code are written for you.  Please remove `eval=FALSE` inside the knitr chunk options for the code to run.

This is not intended to be easy so please come see us to get help.

## Data analysis 

### Pyth!

```{r}
gelman_example_dir<-"http://www.stat.columbia.edu/~gelman/arm/examples/"
pyth <- read.table (paste0(gelman_example_dir,"pyth/exercise2.1.dat"),
                    header=T, sep=" ")
```

The folder pyth contains outcome `y` and inputs `x1`, `x2` for 40 data points, with a further 20 points with the inputs but no observed outcome. Save the file to your working directory and read it into R using the `read.table()` function.

1. Use R to fit a linear regression model predicting `y` from `x1`,`x2`, using the first 40 data points in the file. Summarize the inferences and check the fit of your model.
```{r}
pyth <- read.table(file = "exercise2.1.man", header = T)
attach(pyth)
fx1 <- x1[1:40]
fx2 <- x2[1:40]
fy <- y[1:40]
regout <- lm(fy ~ fx1+fx2) #regression of y on x1 and x2
summary(regout)
```
_The equation of the regression line is $y = 1.3151 + 0.5148x1 + 0.8069x2$. The y intercept is the predicted value for y when both x1 and x2 are zero._ $\beta_{1}$ = 0.5148 _and_ $\beta_{2}$ = 0.8069 _are the differences in the predicted values of y for each unit difference in x1 and x2 respectively.The t-values and the p-values of the regression coefficients show that the coefficients are statistically significant._

_Regarding the fit of the model, the multiple and adjusted R-squared values and also the p-values and F-statistic, all suggest that the model is a very good fit. Almost all the variance of the response variable is explained by the model. However, since R-squared and p-values are not completely reliable measures in isolation, to test the fit of a model, it is good to check the residual plots of the model for any unusual patterns. The residuals are plotted in the third part of this question. _



2. Display the estimated model graphically as in (GH) Figure 3.2.


```{r, fig.align='center', fig.show='hold'}
pyth <- read.table(file = "exercise2.1.man", header = T)
attach(pyth)
fx1 <- x1[1:40]
fx2 <- x2[1:40]
fy <- y[1:40]
regout <- lm(fy ~ fx1+fx2)
fity <- regout$coef[1]+regout$coef[2]*fx1+regout$coef[3]*fx2

#plotting actual vs fitted values
plot(fy,fity, xlab = "Actual Values", ylab = "Fitted Values", pch=16,cex=1.2, col="darkgoldenrod1")
abline(lm(fity~fy), col="midnightblue",lwd=2)

#3D plot of the multiple regression
library(scatterplot3d)
spl <- scatterplot3d(x=fx1,y=fx2,z=fy,pch = 16, angle = 15,type = "p", grid = T, box = F,mar = c(5,5, 0.5, 6),color = "darkgrey")
regl<- lm(fy~fx1+fx2)
spl$plane3d(regl,draw_polygon = TRUE, draw_lines = F)
wh <- resid(regl) > 0 
spl$points3d(fx1[wh], fx2[wh], fy[wh], pch = 19)
```
_In the 3d scatterplot, the dark dots are above the regression plane and the light grey dots are below the regression plane._


3. Make a residual plot for this model. Do the assumptions appear to be met?
```{r}
#residual plot
res <- resid(regout)
df <- data.frame(fx1,fx2,fy)
library(ggplot2)
ggplot(df, aes(res))+
  geom_histogram(aes(fill= ..count..), col="black")+
  scale_fill_gradient("Count", low="green", high="blue")
```
_The residuals do not appear to be normally distributed as expected. The residual plot is skewed to the right._


4. Make predictions for the remaining 20 data points in the file. How confident do you feel about these predictions?
```{r}
train <- pyth[which(pyth$y > 0),  ]
test <- pyth[which(is.na(pyth$y > 0)),  ]

fit <- lm(y~x1+x2, train)
predict(fit)
predict(fit,newdata = test, type = "response")

#confidence intervals for the predictions
CI <- predict(fit,newdata = test, interval = "confidence")
library(knitr)
kable(CI)
```
_The confidence intervals for the predicted values of y seem to be narrow implying that the predictions may be accurate._


After doing this exercise, take a look at Gelman and Nolan (2002, section 9.4) to see where these data came from. (or ask Masanao)

### Earning and height
Suppose that, for a certain population, we can predict log earnings from log height as follows:

- A person who is 66 inches tall is predicted to have earnings of $30,000.
- Every increase of 1% in height corresponds to a predicted increase of 0.8% in earnings.
- The earnings of approximately 95% of people fall within a factor of 1.1 of predicted values.

1. Give the equation of the regression line and the residual standard deviation of the regression.
_y = earnings, x = height and_ $log(y) = \beta_0 + \beta_1log(x)$.
$\beta_0$ = log(30000) - (0.8)*log(66) = 6.9572
_Therefore, the regression equation is log(y) = 6.9572 + 0.8log(x)_

_The earnings of 95% of people fall within 2 standard deviations from the mean._
_(Residual Standard Deviation)*2 = 0.1*0.8_
_Residual sd = 0.04_


2. Suppose the standard deviation of log heights is 5% in this population. What, then, is the $R^2$ of the regression model described here?

_The $R^2$ for the model is_ $1 - (\frac{0.04^2}{0.05^2})$ = 0.36, _which implies that 36% of the variation in y can be explained by the model._

### Beauty and student evaluation 

The folder beauty contains data from Hamermesh and Parker (2005) on student evaluations of instructors' beauty and teaching quality for several courses at the University of Texas. The teaching evaluations were conducted at the end of the semester, and the beauty judgments were made later, by six students who had not attended the classes and were not aware of the course evaluations.
```{r}
beauty.data <- read.table (paste0(gelman_example_dir,"beauty/ProfEvaltnsBeautyPublic.csv"), header=T, sep=",")
```

1. Run a regression using beauty (the variable btystdave) to predict course evaluations (courseevaluation), controlling for various other inputs. Display the fitted model graphically, and explaining the meaning of each of the coefficients, along with the residual standard deviation. Plot the residuals versus fitted values.

```{r}
gelman_example_dir<-"http://www.stat.columbia.edu/~gelman/arm/examples/"
bd <- read.table(paste0(gelman_example_dir,"beauty/ProfEvaltnsBeautyPublic.csv"), header=T, sep=",")

#regression of course evaluation on beauty
fitb <- lm(bd$courseevaluation ~ bd$btystdave)
summary(fitb)
plot(bd$btystdave, bd$courseevaluation, pch=16, col="coral1", xlab = "Beauty", ylab = "Course Evaluations", main = "Regression Plot for Beauty Vs Course Evaluations")
abline(fitb, col="chartreuse4", lwd=2)
df <- data.frame(bd)
ggplot(df, aes(x=predict(fitb), y=resid(fitb)))+ 
  geom_point(col = "darkblue")+
  xlab("Fitted Values")+
  ylab("Residuals")+
  ggtitle("Fitted Values Vs Residuals")
```

2. Fit some other models, including beauty and also other input variables. Consider at least one model with interactions. For each model, state what the predictors are, and what the inputs are, and explain the meaning of each of its coefficients.

_The variables are on differet scales. We need to scale them to compare them._
```{r}
gelman_example_dir<-"http://www.stat.columbia.edu/~gelman/arm/examples/"
bd <- read.table(paste0(gelman_example_dir,"beauty/ProfEvaltnsBeautyPublic.csv"), header=T, sep=",")
bd_sc <- as.data.frame(scale(bd))
fit1 <- lm(bd$courseevaluation ~., bd_sc)
#Performing regression stepwise
stepfit <- stepAIC(fit1, direction = "both", trace = F)
summary(stepfit)

```
_From the summary statistics, the model seems to be a good fit. But, these statistics are not the standalone measures of a good fit._

_Combining the classes into one variable, we can check for interactions between the class variable and the beauty variable. This will be a case of interaction between one binary and one continuous variable._
```{r}
bd$sum_class <- rowSums(bd[,c("class3", "class8", "class12","class14","class17","class18","class19","class26","class27")])
fit2 <- lm(bd$courseevaluation ~ bd$sum_class*bd$btystdave)
summary(fit2)

```
_The regression equation for this model is_ $y = 4.01 - 0.034x_{1} + 0.141x_{2} - 0.194x_{1}:x_{2}$ , _where_ $x_{1}$ _and_ $x_{2}$ _represent the class and beauty variables respectively. Here, the calss variable is binary and the beauty variable is continuous. When both_ $x_{1}$ _and_ $x_{2}$ _are 0, y = intercept = 4.01._

_When_ $x_{1}$ _is 0, the difference in the predicted value of y for a unit change in_ $x_{2}$ _is given by the coefficient of_ $x_{2}$ _= 0.141_

_When_ $x_{2}$ _is 0, the difference in the predicted value of y for a unit change in_ $x_{1}$ _is given by the coefficient of_ $x_{1}$ _= -0.034 (inverse relationship). But the high p-value of this coefficient may imply that the coefficient is not statistically significant and hence, cannot explain the variation in y very effectively._


See also Felton, Mitchell, and Stinson (2003) for more on this topic 
[link](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=426763)

# Conceptula excercises

### On statistical significance.

Note: This is more like a demo to show you that you can get statistically significant result just by random chance. We haven't talked about the significance of the coefficient so we will follow Gelman and use the approximate definition, which is if the estimate is more than 2 sd away from 0 or equivalently, if the z score is bigger than 2 as being "significant".

 ( From Gelman 3.3 ) In this exercise you will simulate two variables that are statistically independent of each other to see what happens when we run a regression of one on the other.  

1. First generate 1000 data points from a normal distribution with mean 0 and standard deviation 1 by typing in R. Generate another variable in the same way (call it var2).

```{r, eval=FALSE}
set.seed(99)
var1 <- rnorm(1000,0,1)
var2 <- rnorm(1000,0,1)
```

Run a regression of one variable on the other. Is the slope coefficient statistically significant? [absolute value of the z-score(the estimated coefficient of var1 divided by its standard error) exceeds 2]

```{r, eval=FALSE}
set.seed(99)
var1 <- rnorm(1000,0,1)
var2 <- rnorm(1000,0,1)
fit  <- lm (var2 ~ var1)
z.scores <- coef(fit)[2]/se.coef(fit)[2]
z.scores
```
_The absolute value of the z-score for the slope coefficient is lesser than 2. Hence, the slope coefficient is not statistically significant._

2. Now run a simulation repeating this process 100 times. This can be done using a loop. From each simulation, save the z-score (the estimated coefficient of var1 divided by its standard error). If the absolute value of the z-score exceeds 2, the estimate is statistically significant. Here is code to perform the simulation:

```{r, eval=FALSE}
z.scores <- rep (NA, 100)
for (k in 1:100) {
  var1 <- rnorm (1000,0,1)
  var2 <- rnorm (1000,0,1)
  fit  <- lm (var2 ~ var1)
  z.scores[k] <- coef(fit)[2]/se.coef(fit)[2]
  z.scores[k]
}
z.scores
z_score <- z.scores[which(abs(z.scores)>2)]
z_score
summary(fit)
```
How many of these 100 z-scores are statistically significant? 
What can you say about statistical significance of regression coefficient?

_Since the variables are generated randomly, the z-scores for each trial are different and hence, there are a different number of statistically significant z-scores with evrry trial. However, the regression coefficient tends to remain statistically insignificant with very small t-values and large p-values._ 
_

### Fit regression removing the effect of other variables

Consider the general multiple-regression equation
$$Y=A+B_1 X_1 + B_2 X_2 +\cdots + B_kX_k+E$$
An alternative procedure for calculating the least-squares coefficient $B_1$ is as follows:

1. Regress $Y$ on $X_2$ through $X_k$, obtaining residuals $E_{Y|2,\dots,k}$.
2. Regress $X_1$ on $X_2$ through $X_k$, obtaining residuals $E_{1|2,\dots,k}$.
3. Regress the residuals $E_{Y|2,\dots,k}$ on the residuals $E_{1|2,\dots,k}$.  The slope for this simple regression is the multiple-regression slope for $X_1$ that is, $B_1$.

(a)  Apply this procedure to the multiple regression of prestige on education, income, and percentage of women in the Canadian occupational prestige data (http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-3E/datasets/Prestige.pdf), confirming that the coefficient for education is properly recovered.

```{r}
fox_data_dir<-"http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-3E/datasets/"
Prestige<-read.table(paste0(fox_data_dir,"Prestige.txt"))
attach(Prestige)

#regression of prestige on other variables excluding education
mreg <- lm(prestige ~ income + women)
yres <- resid(mreg)

#regression of education on the other variables: income and women
xreg <- lm(education ~ income + women)
xres <- resid(xreg)

#regression of the residuals of the above two regressions
ereg <- lm(yres~xres)
ereg
summary(ereg)
```
(b) The intercept for the simple regression in step 3 is 0.  Why is this the case?
_The sum of residuals is equal to zero and the residuals from both the regression models will have  amean of zero. Since the here passes through the mean of the response and predictor variables, the intercept here is zero._


(c) In light of this procedure, is it reasonable to describe $B_1$ as the "effect of $X_1$ on $Y$ when the influence of $X_2,\cdots,X_k$ is removed from both $X_1$ and $Y$"?
$x_{1}$ _appears to explain variability in y not explained by the other variables. Hence, we can consider describing_ $\beta_{1}$ _as the effect of_ $x_{1}$ on $y$ _when influence of other variables is removed._

(d) The procedure in this problem reduces the multiple regression to a series of simple regressions ( in Step 3). Can you see any practical application for this procedure?


### Partial correlation 

The partial correlation between $X_1$ and $Y$ "controlling for" $X_2,\cdots,X_k$ is defined as the simple correlation between the residuals $E_{Y|2,\dots,k}$ and $E_{1|2,\dots,k}$, given in the previous exercise. The partial correlation is denoted $r_{y1|2,\dots, k}$.

1. Using the Canadian occupational prestige data, calculate the partial correlation between prestige and education, controlling for income and percentage women.

```{r}
fox_data_dir<-"http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-3E/datasets/"
Pr<-read.table(paste0(fox_data_dir,"Prestige.txt"))

#Regression of prestige on income and women
reg1 <- lm(prestige ~ income + women, Pr)
res1 <- resid(reg1)
#REgression of education on income and women
reg2 <- lm(education ~ income + women, Pr)
res2 <- resid(reg2)
#Correlation between the residuals
cor(cbind(res2, res1))
```
_The partial correlation between_ $x_{1}$ _and_ $y$ _is_ $r_{y1|2,\dots, k} = 0.736$


2. In light of the interpretation of a partial regression coefficient developed in the previous exercise, why is $r_{y1|2,\dots, k}=0$ if and only if $B_1$ is 0?

## Mathematical exercises.

Prove that the least-squares fit in simple-regression analysis has the following properties:

1. $\sum \hat{y}_i\hat{e}_i =0$

2. $\sum (y_i-\hat{y}_i)(\hat{y}_i-\bar{y}) =\sum \hat{e}_i (\hat{y}_i-\bar{y})=0$

```{r, echo=FALSE}
knitr::include_graphics("C:/Users/GP/Desktop/MEGHA/Appl Stat Modelling/Homework 1/image1.jpg")

```

Suppose that the means and standard deviations of $\mat{y}$ and  $\mat{x}$ are the same:  $\bar{\mat{y}}=\bar{\mat{x}}$ and $sd(\mat{y})=sd(\mat{x})$.
```{r, echo=FALSE}
knitr::include_graphics("C:/Users/GP/Desktop/MEGHA/Appl Stat Modelling/Homework 1/image2.jpg")

```


1. Show that, under these circumstances 
$$\beta_{y|x}=\beta_{x|y}=r_{xy}$$
where $\beta_{y|x}$ is the least-squares slope for the simple regression of $\mat{y}$ on $\mat{x}$, $\beta_{x|y}$ is the least-squares slope for the simple regression of $\mat{x}$ on $\mat{y}$, and $r_{xy}$ is the correlation between the two variables. Show that the intercepts are also the same, $\alpha_{y|x}=\alpha_{x|y}$.

2. Why, if $\alpha_{y|x}=\alpha_{x|y}$ and $\beta_{y|x}=\beta_{x|y}$, is the least squares line for the regression of $\mat{y}$  on $\mat{x}$ different from the line for the regression of $\mat{x}$ on $\mat{y}$ (when $r_{xy}<1$)?

3. Imagine that educational researchers wish to assess the efficacy of a new program to improve the reading performance of children. To test the program, they recruit a group of children who are reading substantially vbelow grade level; after a year in the program, the researchers observe that the children, on average, have imporved their reading performance.  Why is this a weak research design?  How could it be improved?

_The research sample would include only children who are reading below grade. The sample would not be representative of the population, which is all the children. The new program may have a different effect on children who are reading above average and children who are reading at the average level. Overall, the results would be biased._

# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opnions.

